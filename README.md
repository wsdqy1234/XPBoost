# XPBoost
This is the implementation of the paper 'eXtreme Performance Boosting: Revisiting Statistics and Intelligent Optimization in Deep Learning'. We reproduce 8 strong baselines in time-series analysis, including the FCNs, RNNs (LSTM, GRU, LSTNet), CNNs \cite{TCN, WaveNet}, GNNs (GraphWaveNet) and attention models (TPA-LSTM). All the APIs are unified.

You can utilize this as a toolkit to reproduce the papers in time-series domain, or acquire a demo with low development cost. Facing a new scene, you can adapt the model with following steps:
- Define the task in the DataLoader object in the utils.py. The dominant problems in time-series, such as the monitoring, classification and forecasting, can be formulated via the dataloader definition.
- Select the model provided in the 'model' directory, or define a new model following the APIs in the provided models.
- The trainer has been defined in the root directory (in the class '\*Model'). If necessary, re-configure these trainers and hyper-parameters.

So far the following networks are supported :

- FCN: It captures the non-linear relationship between input and output variables. In our experiments, we select the 4-layer FCN with the hidden size 64, 64, 64, 1. There is no difficulty of reproduction, readers can refer to our repository directly for specific implement directly. Other settings follow the default values in the paper. 
- FC-LSTM: It encodes the non-linear auto-relationship and correlationship between the input variables and output variables simultaneously in hidden space. The source code can be found at \url{https://github.com/farizrahman4u/seq2seq}. In our experiments, we utilize the LSTM cell of 100 hidden size. Other settings follow the recommended values in the paper. 
- GRU: It forecasts univariate time-series with fully-connected GRU hidden units, which is a variant of FC-LSTM. The source code can be found at \url{https://github.com/zhangxu0307/time_series_forecasting_pytorch}. In our experiments, we utilize the GRU cell of 100 hidden size.
- LSTNet: It takes advantage of the convolution layer to discover the local dependence patterns among multi-dimensional input variables, and the recurrent layer to capture the long-term dependency patterns. We use the open source code from \url{https://github.com/fbadine/LSTNet}. In our experiments, the number of output filters in the CNN layer is 100, the latent size of the RNN layer is 100, and the CNN filter size is 6. Other experimental settings follow the default values in the paper.
- TCN: It combines the best practices such as the dilations convolution and causal convolution for autoregressive prediction. We take the source code at \url{https://github.com/1ocuslab/TCN}. In our experiments, the kernel size is 2, the numbers of output channels are 32, 64, 128, 128. Other configurations such as the dynamic dilation value follow the default values in the paper. 
- WaveNet: It introduces the skip and residual connection into TCNs, for multi-scale information fusion and fast convergence. We take the source code at \url{https://github.com/LongxingTan/Time-series-prediction}. In experiments, the layer size is 7, the skip size is 1, the numbers of output channels, residual channels and skip channels are 100. Other settings follow the recommended values in the paper. 
- GraphWaveNet: It encodes each node's neighborhood via a low-dimensional embedding by leveraging heat wavelet diffusion patterns. We take the source code at \url{ https://github.com/nnzhan/Graph-WaveNet}.  In our experiments, the number of output channels, residual channels, skip channels and dilation channels are 32, the kernel size is 2, the layer size is 7 and the skip size is 1. we turn on the ‘aptonly’ and 'addaptadj' options to generate adaptive graphs for all datasets. Other configurations follow the options recommended in the paper.
- TPALSTM: It introduces a set of filters to extract time-invariant temporal patterns, which is similar to transform data from time domain into frequency domain. We take the source code at \url{https://github.com/gantheory/TPA-LSTM}. In our experiments, the latent size of the LSTM layer is 100, the number of convolution filter is 100, the kernel size of convolution filter is 6. Other configurations such as the dynamic dilation value follow the default values in the paper.

Further, XPBoost provides a method to boost the performance of deep learning models. Conduct it following the steps as:

- Train the deep learning models using the scripts provided as before.
- Run 'midGen.py' to generate the intermediate datasets.
- Run the meta-optimizers (PSO.py, CS.py, BAS.py) and view the enhanced results.

So far three meta-optimizers are supported by XPBoost:
- Particle Swarm Optimization \cite{pso}: The Particle Swarm Optimization (PSO) utilizes a population of candidate particles, of which the positions are denoted as solutions, to solve optimization problems. These particles are moving in the search-space with a specific velocity, which is updated based on its local best position and global best position among all particles. PSO consists of three important parameters: the momentum $w$, the factor of the global-wise optimum $c_1$, and the factor of particle-wise-optimum $c_2$. In our experiments, we set $w=0.8$, $c_1=0.5$, $c_2=0.5$. The source code can be found from \url{https://github.com/ljvmiranda921/pyswarms} under MIT license. With the collaboration and information sharing between individuals in the population, PSO is possible to help neural network jump out of local optima and approximate the global optimal solution.

- Cuckoo Search \cite{cs}: The Cuckoo Search (CS) algorithm solves optimization problems by simulating the brood parasitism of cuckoos. Specifically, Some species of cockoos dumps their eggs in nests of other host birds of different species. This algorithm uses eggs in nests as a representation of solutions, which are updated based on three steps: a) Update the position of cuckoos based on the best nest and step factor $s$, b) Conduct Levy flight parametrized by the factor $l$ and update the position of nests. c) With a certain possibility $p$ to discover cuckoo eggs and rebuild the nest by host birds, while the number of available nests is maintained. In our experiments, we set $s=0.01$, $\beta=1.5$, $p=0.25$. The source code can be found from \url{https://github.com/ashwinwagh96/SNN-model-using-Cuckoo-Search-Algorithm}.

- Beetle Antennae Search \cite{bas}: The Beetle Antennae Search (BAS) is an algorithm developed based on the behavior of beetle foraging involving antennae searching and random walking. The bettle receive the odours of prey, i.e. the function value, with two tentacles in opposite positions, and tends to move in the direction with a higher concentration of odour. BAS only requires one individual during search and thus greatly simplify calculation compared to population-based algorithms. However, to exploit the potential of this algorithm and make the results comparable, we initialize $N_p$ beetle antennaes and optimize them simultaneously. In our experiments, the distance between the two antennae $d_0=0.2$, $\eta=0.95$. The source code can be found from \url{https://github.com/AAFun/pybas} under GPL-3.0 License.
